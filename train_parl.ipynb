{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Imports**"
      ],
      "metadata": {
        "id": "iak_3weFxIAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as trans\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "from torch.optim import Adam\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "09WrXQ0_frOy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set Device**"
      ],
      "metadata": {
        "id": "Aoh5TdyoxQqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "8eGCZ5vkgmwD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Architecture**"
      ],
      "metadata": {
        "id": "PuRPgKqWxWfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MainConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        super(MainConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride = stride, padding = padding)\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        self.out = out      # to store hidden layer ouput\n",
        "        return out\n",
        "\n",
        "class ResNetBlock(nn.Module):\n",
        "    # ResNet basic block\n",
        "    def __init__(self, in_channels, downsample):\n",
        "        super(ResNetBlock, self).__init__()\n",
        "        if downsample:\n",
        "            stride = 2\n",
        "            out_channels = 2 * in_channels\n",
        "        else:\n",
        "            stride = 1\n",
        "            out_channels = in_channels\n",
        "        self.conv1 = MainConv(in_channels, out_channels, 3, stride = stride, padding = 1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = MainConv(out_channels, out_channels, 3, stride = 1, padding = 1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.convr = nn.Conv2d(in_channels, out_channels, 1, stride = stride, padding = 0) if downsample else None\n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.conv1(x)\n",
        "        y = self.bn1(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.conv2(y)\n",
        "        y = self.bn2(y)\n",
        "        if self.convr is not None:\n",
        "            x = self.convr(x)\n",
        "        return self.relu(x + y)\n",
        "\n",
        "class ResNet20(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(ResNet20, self).__init__()\n",
        "        blocks = [MainConv(3, 16, 3, stride = 1, padding = 1), nn.ReLU(inplace = True)]\n",
        "        in_channels = 16\n",
        "        for i in range(9):\n",
        "            if i > 0 and i % 3 == 0:\n",
        "                blocks.append(ResNetBlock(in_channels, True))\n",
        "                in_channels *= 2\n",
        "            else:\n",
        "                blocks.append(ResNetBlock(in_channels, False))\n",
        "        blocks += [nn.AvgPool2d(8), nn.Flatten(), nn.Linear(in_channels, n_classes)]\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.blocks(x)\n",
        "\n",
        "class PARLModel(nn.Module):\n",
        "    def __init__(self, n_classes = 10, n_ensemble = 3):\n",
        "        # n_ensemble: Number of classifiers in the ensemble\n",
        "        super(PARLModel, self).__init__()\n",
        "        self.nets = nn.ModuleList([ResNet20(n_classes) for _ in range(n_ensemble)])\n",
        "        self.n_ensemble = n_ensemble\n",
        "\n",
        "    def forward(self, x):\n",
        "        return [net(x) for net in self.nets]\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                stdv = 1. / math.sqrt(m.weight.size(1))\n",
        "                m.weight.data.uniform_(-stdv, stdv)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()"
      ],
      "metadata": {
        "id": "dbKfFZcWg06q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Functions**"
      ],
      "metadata": {
        "id": "g6-AUZzfxuHc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4xk_9ApcfgNU"
      },
      "outputs": [],
      "source": [
        "class PARLLoss():\n",
        "    def __init__(self, n_layers, gamma):\n",
        "        # n_layers: first n_layers number of conv layers are used in the loss function\n",
        "        # gamma: hyperparameter controlling the weightage of the penalty term\n",
        "        if n_layers <= 0:\n",
        "            raise ValueError('PARLLoss.n_layers must be a positive integer.')\n",
        "        self.n_layers = n_layers\n",
        "        self.gamma = gamma\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "        self.model = None\n",
        "        self.n_ensemble = None\n",
        "        self.conv_layers = None\n",
        "\n",
        "    def init_loss_calculator(self, model):\n",
        "        self.model = model\n",
        "        self.n_ensemble = model.n_ensemble\n",
        "        ensemble = [model.nets[i].blocks for i in range(model.n_ensemble)]\n",
        "        conv_layers = []\n",
        "        k = 0\n",
        "        for i in range(len(ensemble[0])):\n",
        "            if isinstance(ensemble[0][i], MainConv):\n",
        "                conv_layers.append([ensemble[j][i] for j in range(model.n_ensemble)])\n",
        "                k += 1\n",
        "                if k == self.n_layers:\n",
        "                    break\n",
        "            elif isinstance(ensemble[0][i], ResNetBlock):\n",
        "                conv_layers.append([ensemble[j][i].conv1 for j in range(model.n_ensemble)])\n",
        "                k += 1\n",
        "                if k == self.n_layers:\n",
        "                    break\n",
        "                conv_layers.append([ensemble[j][i].conv2 for j in range(model.n_ensemble)])\n",
        "                k += 1\n",
        "                if k == self.n_layers:\n",
        "                    break\n",
        "        self.conv_layers = conv_layers\n",
        "        print(f'Successfully initialized PARLLoss object. Using {len(self.conv_layers)} layers\\n')\n",
        "\n",
        "    def cosine_sim(self, x, y):\n",
        "        return (x * y).sum() / ((torch.norm(x, p = 2) * torch.norm(y, p = 2)).item() + 1e-12)\n",
        "\n",
        "    def correlation(self, x, y):\n",
        "        n = x.size(dim = -1)\n",
        "        x_centered = x - torch.mean(x, dim = -1, keepdim = True)\n",
        "        y_centered = y - torch.mean(y, dim = -1, keepdim = True)\n",
        "        cov_xy = torch.sum(x_centered * y_centered, dim = -1) / n\n",
        "        corr_xy = cov_xy / (torch.std(x, dim = -1) * torch.std(y, dim = -1) + 1e-12)\n",
        "        return corr_xy.mean()\n",
        "\n",
        "    def __call__(self, X, y_true, train = True):\n",
        "        model_out = self.model(X)\n",
        "        batch_size = X.shape[0]\n",
        "        corr_sum = 0\n",
        "        cosm_sum = 0\n",
        "        for i in range(len(self.conv_layers)):\n",
        "            grads = []\n",
        "            for j in range(self.n_ensemble):\n",
        "                self.conv_layers[i][j].out.backward(torch.ones_like(self.conv_layers[i][j].out), retain_graph = True, create_graph = train)\n",
        "                grads.append(torch.cat([self.conv_layers[k][j].conv.weight.grad.clone().reshape(-1) for k in range(i + 1)]))\n",
        "            self.model.zero_grad()\n",
        "            for j in range(self.n_ensemble - 1):\n",
        "                for k in range(j + 1, self.n_ensemble):\n",
        "                    f1 = torch.permute(self.conv_layers[i][j].out, (1, 2, 3, 0)).reshape(-1, batch_size)\n",
        "                    f2 = torch.permute(self.conv_layers[i][k].out, (1, 2, 3, 0)).reshape(-1, batch_size)\n",
        "                    corr_sum += self.correlation(f1, f2).mean()\n",
        "                    cosm_sum += self.cosine_sim(grads[j], grads[k]).clone()\n",
        "            del grads\n",
        "            torch.cuda.empty_cache()\n",
        "        loss = 0\n",
        "        for y_pred in model_out:\n",
        "              loss += self.loss_fn(y_pred, y_true)\n",
        "        loss /= self.n_ensemble\n",
        "\n",
        "        return loss + (self.gamma * cosm_sum * corr_sum) / self.n_layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Utils**"
      ],
      "metadata": {
        "id": "ZKF9hYEgx11y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataloader(dataset):\n",
        "    if dataset == 'CIFAR-10':\n",
        "        n_classes = 10\n",
        "        train_mean_std = [(0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)]\n",
        "        test_mean_std = [(0.4942, 0.4851, 0.4504), (0.2467, 0.2429, 0.2616)]\n",
        "        dataset_obj = datasets.CIFAR10\n",
        "    elif dataset == 'CIFAR-100':\n",
        "        n_classes = 100\n",
        "        train_mean_std = [(0.5071, 0.4866, 0.4409), (0.2673, 0.2564, 0.2762)]\n",
        "        test_mean_std = [(0.5088, 0.4874, 0.4419), (0.2683, 0.2574, 0.2771)]\n",
        "        dataset_obj = datasets.CIFAR100\n",
        "\n",
        "    train_transforms = trans.Compose([\n",
        "        trans.ToTensor(),\n",
        "        trans.Normalize(*train_mean_std),\n",
        "        trans.RandomCrop(32, padding = 4, padding_mode = 'edge'),\n",
        "        trans.RandomHorizontalFlip()\n",
        "    ])\n",
        "    test_transforms = trans.Compose([\n",
        "        trans.ToTensor(),\n",
        "        trans.Normalize(*test_mean_std)\n",
        "    ])\n",
        "    train_data = dataset_obj(root = dataset, train = True, download = True, transform = train_transforms)\n",
        "    test_data  = dataset_obj(root = dataset, train = False, download = True, transform = test_transforms)\n",
        "    train_dataloader = DataLoader(train_data, batch_size = 64, shuffle = True, num_workers = 4)\n",
        "    test_dataloader  = DataLoader(test_data, batch_size = 64, num_workers = 4)\n",
        "\n",
        "    return n_classes, train_dataloader, test_dataloader\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader, epochs, train_loss_fn, scheduler, optimizer, checkpoint):\n",
        "    model.train()\n",
        "    train_loss_fn.init_loss_calculator(model)\n",
        "    n_batch = len(train_dataloader)\n",
        "    best_val_loss = np.inf\n",
        "\n",
        "    # train loop\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        n_sample_seen = 0\n",
        "        avg_train_loss = 0\n",
        "        start_time = datetime.now()\n",
        "        for batch, (x_train, y_train) in enumerate(train_dataloader):\n",
        "            X, y_true = x_train.to(device), y_train.to(device)\n",
        "            loss = train_loss_fn(X, y_true, train = True)\n",
        "            avg_train_loss = avg_train_loss * n_sample_seen + loss.item()\n",
        "            n_sample_seen += len(y_train)\n",
        "            avg_train_loss /= n_sample_seen\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            for param in model.parameters():\n",
        "                param.grad = None\n",
        "            end_time = datetime.now()\n",
        "            print('\\r', end = '')\n",
        "            print(f\"Epoch: {epoch}/{epochs} | Batch: {batch + 1}/{n_batch} | Learning_Rate: {np.around(scheduler.get_last_lr()[0], 6)} | Training_Loss: {np.around(avg_train_loss, 6)} | Elapsed_Time: {np.around((end_time - start_time).total_seconds(), 1)} s\", end = '', flush = True)\n",
        "        scheduler.step()\n",
        "\n",
        "        # validation\n",
        "        avg_val_loss = 0\n",
        "        n_sample_seen = 0\n",
        "        for x_val, y_val in val_dataloader:\n",
        "            X, y_true = x_val.to(device), y_val.to(device)\n",
        "            loss = train_loss_fn(X, y_true, train = False)\n",
        "            avg_val_loss = avg_val_loss * n_sample_seen + loss.item()\n",
        "            n_sample_seen += len(y_val)\n",
        "            avg_val_loss /= n_sample_seen\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            torch.save(model.state_dict(), checkpoint)\n",
        "            print(f'\\nValidation loss improved from {np.around(best_val_loss, 6)} to {np.around(avg_val_loss, 6)}. Model saved as {checkpoint}.')\n",
        "            best_val_loss = avg_val_loss\n",
        "        else:\n",
        "            print(f'\\nValidation loss did not improve from {np.around(best_val_loss, 6)}.')\n",
        "\n",
        "def evaluate_models(model, dataloader, show_indiv = False):\n",
        "    model.eval()\n",
        "    acc = [0 for _ in range(model.n_ensemble + 1)]\n",
        "    n = 0\n",
        "    for x_test, y_test in dataloader:\n",
        "        X, y_true = x_test.to(device), y_test.to(device)\n",
        "        y_pred_ens = model(X)\n",
        "        y_pred_ens = [torch.argmax(y_pred, -1) for y_pred in y_pred_ens]\n",
        "        y_pred = torch.mode(torch.stack(y_pred_ens, 1)).values\n",
        "        acc[0] += torch.sum(torch.eq(y_pred, y_true)).item()\n",
        "        for i in range(1, model.n_ensemble + 1):\n",
        "            acc[i] += torch.sum(torch.eq(y_pred_ens[i - 1], y_true)).item()\n",
        "        n += len(y_test)\n",
        "\n",
        "    print(f'Combined Ensemble Accuracy: {np.around(100 * acc[0] / n, 2)}%')\n",
        "    if show_indiv:\n",
        "        for i in range(1, model.n_ensemble + 1):\n",
        "            print(f'Classifier-{i} Clean Accuracy: {np.around(100 * acc[i] / n, 2)}%')"
      ],
      "metadata": {
        "id": "IG6f6CPQiuHK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PARL Training Specification**"
      ],
      "metadata": {
        "id": "JdS2cDnAyEdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_ensemble = 3 # Number of classifiers in the ensemble\n",
        "n_layers = 5 # Number of initial conv layers to be considered in PARL loss\n",
        "dataset = 'CIFAR-100' # CIFAR-10, CIFAR-100\n",
        "gamma = 0.25 # Hyperparameter to control the relative importance of the penalty term in PARL loss\n",
        "epochs = 50 # training epochs"
      ],
      "metadata": {
        "id": "VTE2KZJAkU8m"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train**"
      ],
      "metadata": {
        "id": "lipx5tylyIJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes, train_dataloader, test_dataloader = get_dataloader(dataset)\n",
        "model = PARLModel(n_classes = n_classes, n_ensemble = n_ensemble).to(device)\n",
        "model.init_weights()\n",
        "train_loss_fn = PARLLoss(n_layers = n_layers, gamma = gamma)\n",
        "optimizer = Adam(model.parameters(), lr = 0.001)\n",
        "scheduler = MultiStepLR(optimizer, [epochs // 2, 4 * epochs // 5], gamma = 0.1)\n",
        "checkpoint = f'PARL{n_layers}_{dataset}_ResNet20_{n_ensemble}_{epochs}epochs_gamma_{gamma}.pth'\n",
        "\n",
        "# train\n",
        "train(model, train_dataloader, test_dataloader, epochs, train_loss_fn, scheduler, optimizer, checkpoint)\n",
        "\n",
        "# evaluate\n",
        "model.load_state_dict(torch.load(checkpoint))\n",
        "evaluate_models(model, test_dataloader)"
      ],
      "metadata": {
        "id": "LpFdC3p0kRI6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}